{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFCiYA8QwHmf"
      },
      "source": [
        "`Phạm Ngọc Tân`     \n",
        "`19520925`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwvOR_D2qUPr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8pCfXJ1qbkt"
      },
      "source": [
        "S1 = np.array([[2], [10]])\n",
        "S2 = np.array([[10], [-2]])\n",
        "S3 = np.array([[-2], [-10]])\n",
        "S4 = np.array([[-10], [2]])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PHmjrHQvYPa"
      },
      "source": [
        "n_sample = 10\n",
        "red_points = S1 + np.random.normal(0, 1.5, size=(2, n_sample))\n",
        "blue_points = S2 + np.random.normal(0, 1.5, size=(2, n_sample))\n",
        "green_points = S3 + np.random.normal(0, 1.5, size=(2, n_sample))\n",
        "yellow_points = S4 + np.random.normal(0, 1.5, size=(2, n_sample))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vtBU0RPvbBh"
      },
      "source": [
        "x = np.concatenate([blue_points, red_points, green_points, yellow_points], axis=1)\n",
        "x = np.concatenate([np.ones((1,40)), x], axis=0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRbzrY1Pvf-p"
      },
      "source": [
        "a0 = np.array([[1., 0., 0., 0.] for i in range(10)])\n",
        "a1 = np.array([[0., 1., 0., 0.] for i in range(10)])\n",
        "a2 = np.array([[0., 0., 1., 0.] for i in range(10)])\n",
        "a3 = np.array([[0., 0., 0., 1.] for i in range(10)])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVHKmnFuvfZL"
      },
      "source": [
        "y = np.concatenate([a0, a1, a2, a3], axis=0)\n",
        "y = tf.transpose(y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOEHZCOcviwg"
      },
      "source": [
        "theta1 = tf.Variable(np.array([[1., 4., 7.],\n",
        "                              [2., 1., 4.],\n",
        "                              [3., 3., 7.]]))\n",
        "theta2 = tf.Variable(np.array([[1., 4., 7., 4.],\n",
        "                              [2., 1., 4., 5.],\n",
        "                              [3., 3., 7., 9.]]))\n",
        "theta3 = tf.Variable(np.array([[1., 4., 7., 4.],\n",
        "                              [2., 1., 4., 5.],\n",
        "                              [3., 3., 7., 9.],\n",
        "                               [3., 3., 7., 9.]]))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPIaHQdmvkZ6"
      },
      "source": [
        "eps = 0.00001\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CctIx6-qhWw",
        "outputId": "ab6a1ac4-92ed-46cf-8ed0-d920657e9da2"
      },
      "source": [
        "while True:\n",
        "    with tf.GradientTape() as tape:\n",
        "        f1 = tf.linalg.matmul(tf.transpose(theta1), x)\n",
        "        f2 = tf.linalg.matmul(tf.transpose(theta2), tf.math.sigmoid(f1))\n",
        "        f3 = tf.linalg.matmul(tf.transpose(theta3), tf.math.sigmoid(f2))\n",
        "        y_ = tf.nn.softmax(f3, axis=0)\n",
        "        loss = -tf.reduce_mean(y * tf.math.log(y_))\n",
        "        print(\"loss :\", loss.numpy())\n",
        "    grads = tape.gradient(loss, [theta1, theta2, theta3])\n",
        "    print(\"gradient :\", grads[0][0][1].numpy())\n",
        "    opt.apply_gradients(zip(grads, [theta1, theta2, theta3]))\n",
        "    if abs(grads[0][0][1].numpy()) < eps:\n",
        "        break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : 2.231210192828523\n",
            "gradient : -0.0006138452375176012\n",
            "loss : 2.2217308896623016\n",
            "gradient : -0.0006173111578717616\n",
            "loss : 2.212361531914362\n",
            "gradient : -0.000620281605077567\n",
            "loss : 2.2031018934872897\n",
            "gradient : -0.0006227525077145811\n",
            "loss : 2.19395151608218\n",
            "gradient : -0.000624721995462457\n",
            "loss : 2.184909710905376\n",
            "gradient : -0.0006261904059060929\n",
            "loss : 2.17597556240778\n",
            "gradient : -0.0006271602678464773\n",
            "loss : 2.1671479340075934\n",
            "gradient : -0.0006276362614929322\n",
            "loss : 2.1584254757100565\n",
            "gradient : -0.000627625156374952\n",
            "loss : 2.149806633502907\n",
            "gradient : -0.0006271357282473627\n",
            "loss : 2.1412896603748957\n",
            "gradient : -0.0006261786566570251\n",
            "loss : 2.1328726287778332\n",
            "gradient : -0.0006247664051804885\n",
            "loss : 2.1245534443308567\n",
            "gradient : -0.0006229130866206215\n",
            "loss : 2.116329860549623\n",
            "gradient : -0.000620634315659977\n",
            "loss : 2.1081994943729754\n",
            "gradient : -0.0006179470516051702\n",
            "loss : 2.1001598422555547\n",
            "gradient : -0.0006148694339201986\n",
            "loss : 2.0922082965964024\n",
            "gradient : -0.0006114206132391431\n",
            "loss : 2.084342162280553\n",
            "gradient : -0.0006076205804754026\n",
            "loss : 2.0765586731223205\n",
            "gradient : -0.0006034899965128534\n",
            "loss : 2.0688550080146575\n",
            "gradient : -0.0005990500247817649\n",
            "loss : 2.061228306607943\n",
            "gradient : -0.0005943221688005268\n",
            "loss : 2.053675684362856\n",
            "gradient : -0.000589328116511507\n",
            "loss : 2.046194246844881\n",
            "gradient : -0.0005840895929679922\n",
            "loss : 2.038781103151605\n",
            "gradient : -0.0005786282226472708\n",
            "loss : 2.0314333783876237\n",
            "gradient : -0.0005729654023832234\n",
            "loss : 2.024148225124859\n",
            "gradient : -0.0005671221856367425\n",
            "loss : 2.016922833807951\n",
            "gradient : -0.0005611191785620651\n",
            "loss : 2.0097544420845805\n",
            "gradient : -0.0005549764480856932\n",
            "loss : 2.002640343058921\n",
            "gradient : -0.0005487134419971461\n",
            "loss : 1.9955778924825296\n",
            "gradient : -0.0005423489208591342\n",
            "loss : 1.9885645149109314\n",
            "gradient : -0.0005359009013806252\n",
            "loss : 1.981597708865732\n",
            "gradient : -0.0005293866107601653\n",
            "loss : 1.974675051051483\n",
            "gradient : -0.000522822451397759\n",
            "loss : 1.9677941996837576\n",
            "gradient : -0.0005162239752908048\n",
            "loss : 1.9609528969901333\n",
            "gradient : -0.0005096058673705918\n",
            "loss : 1.954148970949221\n",
            "gradient : -0.0005029819369990316\n",
            "loss : 1.947380336334699\n",
            "gradient : -0.0004963651168275801\n",
            "loss : 1.940644995131769\n",
            "gradient : -0.0004897674682194727\n",
            "loss : 1.9339410363926617\n",
            "gradient : -0.00048320019244959856\n",
            "loss : 1.927266635596131\n",
            "gradient : -0.0004766736469212261\n",
            "loss : 1.920620053573326\n",
            "gradient : -0.00047019736567256265\n",
            "loss : 1.9139996350593382\n",
            "gradient : -0.0004637800834873333\n",
            "loss : 1.9074038069261356\n",
            "gradient : -0.00045742976296917647\n",
            "loss : 1.9008310761487457\n",
            "gradient : -0.00045115362398903745\n",
            "loss : 1.8942800275524996\n",
            "gradient : -0.0004449581749654456\n",
            "loss : 1.8877493213850376\n",
            "gradient : -0.0004388492454889896\n",
            "loss : 1.8812376907526756\n",
            "gradient : -0.000432832019853021\n",
            "loss : 1.8747439389567142\n",
            "gradient : -0.00042691107110195026\n",
            "loss : 1.8682669367614024\n",
            "gradient : -0.0004210903952557915\n",
            "loss : 1.8618056196215627\n",
            "gradient : -0.0004153734454144289\n",
            "loss : 1.8553589848944092\n",
            "gradient : -0.0004097631654868876\n",
            "loss : 1.8489260890568417\n",
            "gradient : -0.00040426202332957727\n",
            "loss : 1.8425060449464759\n",
            "gradient : -0.00039887204311351535\n",
            "loss : 1.8360980190419312\n",
            "gradient : -0.0003935948367722111\n",
            "loss : 1.8297012287953625\n",
            "gradient : -0.0003884316344118998\n",
            "loss : 1.8233149400279636\n",
            "gradient : -0.00038338331359141824\n",
            "loss : 1.8169384643971314\n",
            "gradient : -0.0003784504274022556\n",
            "loss : 1.8105711569421565\n",
            "gradient : -0.0003736332313001855\n",
            "loss : 1.8042124137136977\n",
            "gradient : -0.00036893170865703347\n",
            "loss : 1.797861669490891\n",
            "gradient : -0.0003643455950172185\n",
            "loss : 1.7915183955887017\n",
            "gradient : -0.00035987440105645355\n",
            "loss : 1.7851820977570565\n",
            "gradient : -0.00035551743425143404\n",
            "loss : 1.7788523141723929\n",
            "gradient : -0.0003512738192787303\n",
            "loss : 1.77252861352145\n",
            "gradient : -0.00034714251716885927\n",
            "loss : 1.7662105931764915\n",
            "gradient : -0.00034312234324777716\n",
            "loss : 1.7598978774605882\n",
            "gradient : -0.0003392119839032132\n",
            "loss : 1.7535901160011318\n",
            "gradient : -0.0003354100122170146\n",
            "loss : 1.7472869821693966\n",
            "gradient : -0.00033171490250783446\n",
            "loss : 1.7409881716036626\n",
            "gradient : -0.00032812504383028\n",
            "loss : 1.7346934008131882\n",
            "gradient : -0.000324638752478327\n",
            "loss : 1.7284024058601677\n",
            "gradient : -0.00032125428354112427\n",
            "loss : 1.722114941116672\n",
            "gradient : -0.00031796984155992405\n",
            "loss : 1.7158307780935047\n",
            "gradient : -0.00031478359033423295\n",
            "loss : 1.7095497043378713\n",
            "gradient : -0.00031169366192482543\n",
            "loss : 1.7032715223967407\n",
            "gradient : -0.0003086981649004495\n",
            "loss : 1.6969960488428064\n",
            "gradient : -0.0003057951918734577\n",
            "loss : 1.6907231133599807\n",
            "gradient : -0.0003029828263687574\n",
            "loss : 1.6844525578854301\n",
            "gradient : -0.0003002591490684761\n",
            "loss : 1.6781842358052117\n",
            "gradient : -0.0002976222434734744\n",
            "loss : 1.6719180112006633\n",
            "gradient : -0.00029507020102073557\n",
            "loss : 1.6656537581427937\n",
            "gradient : -0.00029260112569441844\n",
            "loss : 1.6593913600320014\n",
            "gradient : -0.00029021313816602164\n",
            "loss : 1.6531307089805687\n",
            "gradient : -0.00028790437949772444\n",
            "loss : 1.6468717052354733\n",
            "gradient : -0.0002856730144409863\n",
            "loss : 1.6406142566391675\n",
            "gradient : -0.0002835172343608246\n",
            "loss : 1.6343582781260841\n",
            "gradient : -0.00028143525981439235\n",
            "loss : 1.6281036912527267\n",
            "gradient : -0.00027942534281090713\n",
            "loss : 1.6218504237593243\n",
            "gradient : -0.00027748576877828404\n",
            "loss : 1.6155984091611124\n",
            "gradient : -0.0002756148582601663\n",
            "loss : 1.6093475863674152\n",
            "gradient : -0.00027381096836591283\n",
            "loss : 1.6030978993268044\n",
            "gradient : -0.00027207249399411465\n",
            "loss : 1.5968492966966987\n",
            "gradient : -0.0002703978688493365\n",
            "loss : 1.590601731535858\n",
            "gradient : -0.00026878556627013604\n",
            "loss : 1.5843551610183244\n",
            "gradient : -0.0002672340998853019\n",
            "loss : 1.5781095461674355\n",
            "gradient : -0.000265742024114121\n",
            "loss : 1.5718648516086258\n",
            "gradient : -0.0002643079345252359\n",
            "loss : 1.565621045339801\n",
            "gradient : -0.0002629304680676359\n",
            "loss : 1.5593780985181518\n",
            "gradient : -0.0002616083031864913\n",
            "loss : 1.5531359852623383\n",
            "gradient : -0.0002603401598352125\n",
            "loss : 1.5468946824690417\n",
            "gradient : -0.0002591247993948654\n",
            "loss : 1.5406541696429503\n",
            "gradient : -0.0002579610245104662\n",
            "loss : 1.5344144287392971\n",
            "gradient : -0.0002568476788536372\n",
            "loss : 1.5281754440181323\n",
            "gradient : -0.000255783646819837\n",
            "loss : 1.5219372019095572\n",
            "gradient : -0.00025476785316811724\n",
            "loss : 1.5156996908892058\n",
            "gradient : -0.0002537992626103138\n",
            "loss : 1.5094629013633019\n",
            "gradient : -0.0002528768793564046\n",
            "loss : 1.5032268255626648\n",
            "gradient : -0.0002519997466218614\n",
            "loss : 1.4969914574450827\n",
            "gradient : -0.00025116694610261264\n",
            "loss : 1.490756792605504\n",
            "gradient : -0.0002503775974224603\n",
            "loss : 1.4845228281935519\n",
            "gradient : -0.0002496308575576279\n",
            "loss : 1.47828956283787\n",
            "gradient : -0.0002489259202425027\n",
            "loss : 1.472056996576877\n",
            "gradient : -0.00024826201536037694\n",
            "loss : 1.4658251307955188\n",
            "gradient : -0.00024763840832258305\n",
            "loss : 1.4595939681676273\n",
            "gradient : -0.0002470543994390318\n",
            "loss : 1.4533635126035465\n",
            "gradient : -0.0002465093232830322\n",
            "loss : 1.44713376920269\n",
            "gradient : -0.00024600254805287543\n",
            "loss : 1.4409047442107294\n",
            "gradient : -0.0002455334749322275\n",
            "loss : 1.4346764449811342\n",
            "gradient : -0.00024510153745159707\n",
            "loss : 1.4284488799407946\n",
            "gradient : -0.0002447062008524185\n",
            "loss : 1.422222058559497\n",
            "gradient : -0.00024434696145544206\n",
            "loss : 1.4159959913230202\n",
            "gradient : -0.00024402334603473635\n",
            "loss : 1.4097706897096551\n",
            "gradient : -0.00024373491119843938\n",
            "loss : 1.403546166169947\n",
            "gradient : -0.00024348124277746852\n",
            "loss : 1.3973224341095007\n",
            "gradient : -0.00024326195522287745\n",
            "loss : 1.3910995078746744\n",
            "gradient : -0.0002430766910126661\n",
            "loss : 1.384877402741024\n",
            "gradient : -0.00024292512006869493\n",
            "loss : 1.3786561349043613\n",
            "gradient : -0.00024280693918418128\n",
            "loss : 1.3724357214743\n",
            "gradient : -0.0002427218714620678\n",
            "loss : 1.3662161804701805\n",
            "gradient : -0.00024266966576469532\n",
            "loss : 1.3599975308192729\n",
            "gradient : -0.00024265009617482637\n",
            "loss : 1.353779792357161\n",
            "gradient : -0.00024266296146826377\n",
            "loss : 1.347562985830231\n",
            "gradient : -0.00024270808459790952\n",
            "loss : 1.3413471329001854\n",
            "gradient : -0.00024278531218936193\n",
            "loss : 1.335132256150517\n",
            "gradient : -0.00024289451404786345\n",
            "loss : 1.3289183790948866\n",
            "gradient : -0.00024303558267638726\n",
            "loss : 1.322705526187352\n",
            "gradient : -0.00024320843280462186\n",
            "loss : 1.3164937228343994\n",
            "gradient : -0.0002434130009286232\n",
            "loss : 1.3102829954087476\n",
            "gradient : -0.00024364924486056785\n",
            "loss : 1.3040733712648842\n",
            "gradient : -0.00024391714328850185\n",
            "loss : 1.2978648787563163\n",
            "gradient : -0.00024421669534527085\n",
            "loss : 1.2916575472545047\n",
            "gradient : -0.0002445479201863852\n",
            "loss : 1.2854514071694787\n",
            "gradient : -0.0002449108565761829\n",
            "loss : 1.2792464899721119\n",
            "gradient : -0.00024530556248162597\n",
            "loss : 1.2730428282180557\n",
            "gradient : -0.00024573211467319664\n",
            "loss : 1.2668404555733352\n",
            "gradient : -0.00024619060833211073\n",
            "loss : 1.2606394068416042\n",
            "gradient : -0.000246681156663244\n",
            "loss : 1.254439717993073\n",
            "gradient : -0.0002472038905129189\n",
            "loss : 1.2482414261951194\n",
            "gradient : -0.00024775895799079026\n",
            "loss : 1.2420445698445983\n",
            "gradient : -0.00024834652409500213\n",
            "loss : 1.235849188601875\n",
            "gradient : -0.0002489667703397063\n",
            "loss : 1.2296553234266012\n",
            "gradient : -0.00024961989438415134\n",
            "loss : 1.223463016615265\n",
            "gradient : -0.00025030610966217317\n",
            "loss : 1.217272311840549\n",
            "gradient : -0.00025102564501133985\n",
            "loss : 1.2110832541925274\n",
            "gradient : -0.00025177874430057026\n",
            "loss : 1.2048958902217417\n",
            "gradient : -0.00025256566605520215\n",
            "loss : 1.1987102679842063\n",
            "gradient : -0.00025338668307842285\n",
            "loss : 1.1925264370883752\n",
            "gradient : -0.0002542420820678514\n",
            "loss : 1.1863444487441364\n",
            "gradient : -0.0002551321632262006\n",
            "loss : 1.180164355813869\n",
            "gradient : -0.0002560572398646015\n",
            "loss : 1.173986212865639\n",
            "gradient : -0.00025701763799749106\n",
            "loss : 1.1678100762285761\n",
            "gradient : -0.00025801369592762995\n",
            "loss : 1.1616360040505036\n",
            "gradient : -0.00025904576381986327\n",
            "loss : 1.1554640563578875\n",
            "gradient : -0.00026011420326225033\n",
            "loss : 1.1492942951181726\n",
            "gradient : -0.0002612193868131209\n",
            "loss : 1.143126784304576\n",
            "gradient : -0.00026236169753235524\n",
            "loss : 1.1369615899634198\n",
            "gradient : -0.0002635415284955796\n",
            "loss : 1.1307987802840787\n",
            "gradient : -0.00026475928228930504\n",
            "loss : 1.1246384256716222\n",
            "gradient : -0.0002660153704856601\n",
            "loss : 1.1184805988222433\n",
            "gradient : -0.00026731021309471596\n",
            "loss : 1.1123253748015571\n",
            "gradient : -0.00026864423799266704\n",
            "loss : 1.1061728311258636\n",
            "gradient : -0.0002700178803240492\n",
            "loss : 1.1000230478464654\n",
            "gradient : -0.0002714315818759686\n",
            "loss : 1.0938761076371446\n",
            "gradient : -0.0002728857904224202\n",
            "loss : 1.0877320958848888\n",
            "gradient : -0.000274380959036616\n",
            "loss : 1.0815911007839811\n",
            "gradient : -0.0002759175453691492\n",
            "loss : 1.0754532134335504\n",
            "gradient : -0.0002774960108898677\n",
            "loss : 1.069318527938692\n",
            "gradient : -0.00027911682009120555\n",
            "loss : 1.0631871415152692\n",
            "gradient : -0.000280780439650523\n",
            "loss : 1.057059154598514\n",
            "gradient : -0.00028248733754933705\n",
            "loss : 1.0509346709555258\n",
            "gradient : -0.00028423798214661833\n",
            "loss : 1.0448137978018024\n",
            "gradient : -0.0002860328412040503\n",
            "loss : 1.0386966459219065\n",
            "gradient : -0.0002878723808603189\n",
            "loss : 1.0325833297943925\n",
            "gradient : -0.00028975706455205405\n",
            "loss : 1.0264739677211114\n",
            "gradient : -0.0002916873518785592\n",
            "loss : 1.0203686819610132\n",
            "gradient : -0.0002936636974077343\n",
            "loss : 1.014267598868563\n",
            "gradient : -0.00029568654942020256\n",
            "loss : 1.008170849036898\n",
            "gradient : -0.0002977563485890567\n",
            "loss : 1.0020785674458315\n",
            "gradient : -0.0002998735265920852\n",
            "loss : 0.9959908936148274\n",
            "gradient : -0.0003020385046537456\n",
            "loss : 0.9899079717610595\n",
            "gradient : -0.00030425169201387113\n",
            "loss : 0.9838299509626595\n",
            "gradient : -0.0003065134843201424\n",
            "loss : 0.9777569853272639\n",
            "gradient : -0.0003088242619414234\n",
            "loss : 0.9716892341659659\n",
            "gradient : -0.00031118438819894163\n",
            "loss : 0.9656268621727552\n",
            "gradient : -0.00031359420751238195\n",
            "loss : 0.9595700396095506\n",
            "gradient : -0.00031605404345802057\n",
            "loss : 0.9535189424968891\n",
            "gradient : -0.0003185641967359715\n",
            "loss : 0.9474737528103576\n",
            "gradient : -0.0003211249430437941\n",
            "loss : 0.9414346586828151\n",
            "gradient : -0.0003237365308537091\n",
            "loss : 0.93540185461246\n",
            "gradient : -0.00032639917909087814\n",
            "loss : 0.9293755416767737\n",
            "gradient : -0.00032911307471012943\n",
            "loss : 0.9233559277523551\n",
            "gradient : -0.0003318783701689436\n",
            "loss : 0.917343227740648\n",
            "gradient : -0.00033469518079449096\n",
            "loss : 0.9113376637995415\n",
            "gradient : -0.00033756358204280434\n",
            "loss : 0.9053394655807916\n",
            "gradient : -0.0003404836066483847\n",
            "loss : 0.8993488704732053\n",
            "gradient : -0.0003434552416628681\n",
            "loss : 0.8933661238514837\n",
            "gradient : -0.00034647842538175467\n",
            "loss : 0.8873914793305959\n",
            "gradient : -0.0003495530441584133\n",
            "loss : 0.8814251990255274\n",
            "gradient : -0.0003526789291050792\n",
            "loss : 0.8754675538162001\n",
            "gradient : -0.00035585585268119966\n",
            "loss : 0.8695188236173248\n",
            "gradient : -0.00035908352516964665\n",
            "loss : 0.8635792976529043\n",
            "gradient : -0.00036236159104223556\n",
            "loss : 0.857649274735056\n",
            "gradient : -0.00036568962521640473\n",
            "loss : 0.8517290635467637\n",
            "gradient : -0.00036906712920576094\n",
            "loss : 0.8458189829281272\n",
            "gradient : -0.00037249352716786974\n",
            "loss : 0.8399193621655957\n",
            "gradient : -0.0003759681618535551\n",
            "loss : 0.8340305412836269\n",
            "gradient : -0.00037949029046290163\n",
            "loss : 0.8281528713381231\n",
            "gradient : -0.000383059080414159\n",
            "loss : 0.8222867147109328\n",
            "gradient : -0.00038667360503273396\n",
            "loss : 0.8164324454046227\n",
            "gradient : -0.0003903328391688055\n",
            "loss : 0.8105904493366249\n",
            "gradient : -0.0003940356547530949\n",
            "loss : 0.8047611246317995\n",
            "gradient : -0.00039778081630193705\n",
            "loss : 0.7989448819123199\n",
            "gradient : -0.00040156697638394316\n",
            "loss : 0.793142144583716\n",
            "gradient : -0.00040539267106229284\n",
            "loss : 0.787353349115785\n",
            "gradient : -0.00040925631532819453\n",
            "loss : 0.7815789453169677\n",
            "gradient : -0.0004131561985425585\n",
            "loss : 0.7758193966006709\n",
            "gradient : -0.00041709047990518217\n",
            "loss : 0.7700751802418964\n",
            "gradient : -0.0004210571839719235\n",
            "loss : 0.7643467876223994\n",
            "gradient : -0.00042505419624291635\n",
            "loss : 0.7586347244624789\n",
            "gradient : -0.00042907925884628427\n",
            "loss : 0.7529395110373531\n",
            "gradient : -0.0004331299663443599\n",
            "loss : 0.7472616823759484\n",
            "gradient : -0.00043720376169100134\n",
            "loss : 0.7416017884397833\n",
            "gradient : -0.00044129793237118595\n",
            "loss : 0.7359603942794861\n",
            "gradient : -0.0004454096067557475\n",
            "loss : 0.730338080166355\n",
            "gradient : -0.000449535750706567\n",
            "loss : 0.7247354416962185\n",
            "gradient : -0.00045367316446944016\n",
            "loss : 0.7191530898627322\n",
            "gradient : -0.00045781847989392205\n",
            "loss : 0.7135916510971088\n",
            "gradient : -0.0004619681580214575\n",
            "loss : 0.7080517672711656\n",
            "gradient : -0.0004661184870848483\n",
            "loss : 0.7025340956604536\n",
            "gradient : -0.00047026558096392913\n",
            "loss : 0.6970393088641386\n",
            "gradient : -0.0004744053781438522\n",
            "loss : 0.6915680946782148\n",
            "gradient : -0.00047853364122345547\n",
            "loss : 0.6861211559185714\n",
            "gradient : -0.00048264595702272856\n",
            "loss : 0.680699210190369\n",
            "gradient : -0.0004867377373384455\n",
            "loss : 0.6753029896001845\n",
            "gradient : -0.0004908042203982428\n",
            "loss : 0.669933240407358\n",
            "gradient : -0.0004948404730625336\n",
            "loss : 0.6645907226110347\n",
            "gradient : -0.0004988413938239918\n",
            "loss : 0.659276209469445\n",
            "gradient : -0.0005028017166527916\n",
            "loss : 0.6539904869480861\n",
            "gradient : -0.0005067160157345345\n",
            "loss : 0.6487343530936007\n",
            "gradient : -0.0005105787111457047\n",
            "loss : 0.643508617330361\n",
            "gradient : -0.000514384075508568\n",
            "loss : 0.6383140996769858\n",
            "gradient : -0.0005181262416640474\n",
            "loss : 0.6331516298803248\n",
            "gradient : -0.00052179921139698\n",
            "loss : 0.628022046464787\n",
            "gradient : -0.0005253968652431083\n",
            "loss : 0.6229261956952863\n",
            "gradient : -0.0005289129734014141\n",
            "loss : 0.6178649304525476\n",
            "gradient : -0.0005323412077689514\n",
            "loss : 0.6128391090200265\n",
            "gradient : -0.0005356751551077729\n",
            "loss : 0.6078495937822797\n",
            "gradient : -0.0005389083313456213\n",
            "loss : 0.6028972498352557\n",
            "gradient : -0.0005420341970028652\n",
            "loss : 0.5979829435096773\n",
            "gradient : -0.0005450461737285597\n",
            "loss : 0.5931075408094204\n",
            "gradient : -0.0005479376619180926\n",
            "loss : 0.5882719057676058\n",
            "gradient : -0.0005507020593737867\n",
            "loss : 0.5834768987239556\n",
            "gradient : -0.0005533327809582734\n",
            "loss : 0.578723374527845\n",
            "gradient : -0.0005558232791783763\n",
            "loss : 0.574012180672388\n",
            "gradient : -0.0005581670656249476\n",
            "loss : 0.5693441553658365\n",
            "gradient : -0.0005603577331815014\n",
            "loss : 0.5647201255474916\n",
            "gradient : -0.0005623889789021153\n",
            "loss : 0.56014090485628\n",
            "gradient : -0.0005642546274463997\n",
            "loss : 0.5556072915610557\n",
            "gradient : -0.0005659486549477131\n",
            "loss : 0.5511200664625823\n",
            "gradient : -0.0005674652131789842\n",
            "loss : 0.5466799907779883\n",
            "gradient : -0.0005687986538700704\n",
            "loss : 0.5422878040192752\n",
            "gradient : -0.0005699435530206878\n",
            "loss : 0.5379442218781518\n",
            "gradient : -0.0005708947350446109\n",
            "loss : 0.5336499341300871\n",
            "gradient : -0.0005716472965735591\n",
            "loss : 0.5294056025709653\n",
            "gradient : -0.000572196629743839\n",
            "loss : 0.525211859000106\n",
            "gradient : -0.0005725384447850737\n",
            "loss : 0.5210693032636468\n",
            "gradient : -0.0005726687917285379\n",
            "loss : 0.5169785013723736\n",
            "gradient : -0.0005725840810531332\n",
            "loss : 0.512939983708012\n",
            "gradient : -0.0005722811030894652\n",
            "loss : 0.5089542433317462\n",
            "gradient : -0.0005717570460075338\n",
            "loss : 0.5050217344083292\n",
            "gradient : -0.0005710095122206067\n",
            "loss : 0.5011428707585458\n",
            "gradient : -0.0005700365330473946\n",
            "loss : 0.4973180245520397\n",
            "gradient : -0.0005688365814866066\n",
            "loss : 0.49354752515157935\n",
            "gradient : -0.0005674085829717052\n",
            "loss : 0.48983165811874907\n",
            "gradient : -0.0005657519239899432\n",
            "loss : 0.4861706643898042\n",
            "gradient : -0.0005638664584674358\n",
            "loss : 0.48256473962907087\n",
            "gradient : -0.00056175251184175\n",
            "loss : 0.4790140337657518\n",
            "gradient : -0.0005594108827641931\n",
            "loss : 0.4755186507184394\n",
            "gradient : -0.0005568428423960097\n",
            "loss : 0.47207864830995466\n",
            "gradient : -0.0005540501312855078\n",
            "loss : 0.4686940383734253\n",
            "gradient : -0.0005510349538360142\n",
            "loss : 0.46536478704878437\n",
            "gradient : -0.0005477999703980042\n",
            "loss : 0.46209081526713147\n",
            "gradient : -0.0005443482870413014\n",
            "loss : 0.4588719994187018\n",
            "gradient : -0.0005406834430858664\n",
            "loss : 0.45570817219853826\n",
            "gradient : -0.0005368093964903085\n",
            "loss : 0.4525991236223964\n",
            "gradient : -0.00053273050721778\n",
            "loss : 0.44954460220395215\n",
            "gradient : -0.0005284515187165506\n",
            "loss : 0.4465443162830388\n",
            "gradient : -0.0005239775376693165\n",
            "loss : 0.44359793549345633\n",
            "gradient : -0.000519314012179356\n",
            "loss : 0.44070509235785016\n",
            "gradient : -0.0005144667085736743\n",
            "loss : 0.4378653839963002\n",
            "gradient : -0.0005094416870127206\n",
            "loss : 0.4350783739345707\n",
            "gradient : -0.0005042452761034124\n",
            "loss : 0.43234359399747496\n",
            "gradient : -0.000498884046716471\n",
            "loss : 0.4296605462724896\n",
            "gradient : -0.0004933647852113588\n",
            "loss : 0.42702870512862356\n",
            "gradient : -0.0004876944662711359\n",
            "loss : 0.42444751927559243\n",
            "gradient : -0.0004818802255471171\n",
            "loss : 0.42191641384856454\n",
            "gradient : -0.00047592933230765095\n",
            "loss : 0.4194347925041205\n",
            "gradient : -0.0004698491622785034\n",
            "loss : 0.4170020395135923\n",
            "gradient : -0.00046364717085301454\n",
            "loss : 0.4146175218405917\n",
            "gradient : -0.00045733086683965126\n",
            "loss : 0.41228059119031757\n",
            "gradient : -0.0004509077869023819\n",
            "loss : 0.4099905860190797\n",
            "gradient : -0.0004443854708361572\n",
            "loss : 0.4077468334934376\n",
            "gradient : -0.00043777143780549785\n",
            "loss : 0.40554865138934415\n",
            "gradient : -0.00043107316365950844\n",
            "loss : 0.4033953499227404\n",
            "gradient : -0.000424298059421301\n",
            "loss : 0.40128623350412224\n",
            "gradient : -0.0004174534510346835\n",
            "loss : 0.39922060241068696\n",
            "gradient : -0.0004105465604352417\n",
            "loss : 0.39719775437074933\n",
            "gradient : -0.0004035844879982918\n",
            "loss : 0.39521698605618194\n",
            "gradient : -0.00039657419640129075\n",
            "loss : 0.39327759447966953\n",
            "gradient : -0.0003895224959242119\n",
            "loss : 0.39137887829455437\n",
            "gradient : -0.0003824360311983438\n",
            "loss : 0.3895201389960035\n",
            "gradient : -0.00037532126940105276\n",
            "loss : 0.38770068202310004\n",
            "gradient : -0.00036818448988300815\n",
            "loss : 0.38591981776229256\n",
            "gradient : -0.0003610317752033669\n",
            "loss : 0.3841768624533829\n",
            "gradient : -0.0003538690035392243\n",
            "loss : 0.3824711389999186\n",
            "gradient : -0.0003467018424270251\n",
            "loss : 0.38080197768646035\n",
            "gradient : -0.00033953574378644576\n",
            "loss : 0.3791687168057374\n",
            "gradient : -0.0003323759401707173\n",
            "loss : 0.3775707031991569\n",
            "gradient : -0.00032522744218253287\n",
            "loss : 0.37600729271453537\n",
            "gradient : -0.0003180950369899352\n",
            "loss : 0.3744778505852338\n",
            "gradient : -0.0003109832878739127\n",
            "loss : 0.37298175173513964\n",
            "gradient : -0.00030389653473661995\n",
            "loss : 0.3715183810141333\n",
            "gradient : -0.00029683889549784564\n",
            "loss : 0.37008713336880905\n",
            "gradient : -0.0002898142683066924\n",
            "loss : 0.368687413953305\n",
            "gradient : -0.0002828263344953478\n",
            "loss : 0.3673186381851279\n",
            "gradient : -0.0002758785622024463\n",
            "loss : 0.3659802317508441\n",
            "gradient : -0.00026897421059492003\n",
            "loss : 0.364671630566455\n",
            "gradient : -0.00026211633461868347\n",
            "loss : 0.3633922806971843\n",
            "gradient : -0.0002553077902110012\n",
            "loss : 0.3621416382412831\n",
            "gradient : -0.0002485512399096062\n",
            "loss : 0.36091916918230915\n",
            "gradient : -0.00024184915879656168\n",
            "loss : 0.359724349214166\n",
            "gradient : -0.00023520384071811222\n",
            "loss : 0.3585566635429921\n",
            "gradient : -0.0002286174047246961\n",
            "loss : 0.35741560666978617\n",
            "gradient : -0.00022209180167900904\n",
            "loss : 0.3563006821574336\n",
            "gradient : -0.0002156288209832964\n",
            "loss : 0.3552114023855711\n",
            "gradient : -0.00020923009738073463\n",
            "loss : 0.3541472882964885\n",
            "gradient : -0.0002028971177891418\n",
            "loss : 0.35310786913503317\n",
            "gradient : -0.0001966312281289914\n",
            "loss : 0.35209268218523726\n",
            "gradient : -0.0001904336401110751\n",
            "loss : 0.3511012725061528\n",
            "gradient : -0.0001843054379525212\n",
            "loss : 0.3501331926691428\n",
            "gradient : -0.00017824758499332456\n",
            "loss : 0.34918800249864324\n",
            "gradient : -0.0001722609301885614\n",
            "loss : 0.34826526881818787\n",
            "gradient : -0.00016634621445473812\n",
            "loss : 0.3473645652032659\n",
            "gradient : -0.00016050407685139852\n",
            "loss : 0.34648547174237737\n",
            "gradient : -0.00015473506058207728\n",
            "loss : 0.3456275748074439\n",
            "gradient : -0.00014903961880124216\n",
            "loss : 0.3447904668345465\n",
            "gradient : -0.00014341812021622288\n",
            "loss : 0.3439737461157784\n",
            "gradient : -0.00013787085447560765\n",
            "loss : 0.3431770166028289\n",
            "gradient : -0.00013239803733749555\n",
            "loss : 0.34239988772276403\n",
            "gradient : -0.00012699981561304873\n",
            "loss : 0.3416419742063075\n",
            "gradient : -0.00012167627188267403\n",
            "loss : 0.3409028959288035\n",
            "gradient : -0.00011642742898357612\n",
            "loss : 0.3401822777639093\n",
            "gradient : -0.00011125325426913742\n",
            "loss : 0.33947974944995335\n",
            "gradient : -0.0001061536636417224\n",
            "loss : 0.33879494546879435\n",
            "gradient : -0.00010112852536184494\n",
            "loss : 0.33812750493692223\n",
            "gradient : -9.617766363757742e-05\n",
            "loss : 0.33747707150845985\n",
            "gradient : -9.130086199903627e-05\n",
            "loss : 0.33684329328965634\n",
            "gradient : -8.649786646360744e-05\n",
            "loss : 0.33622582276439966\n",
            "gradient : -8.17683884981993e-05\n",
            "loss : 0.3356243167302219\n",
            "gradient : -7.71121077853934e-05\n",
            "loss : 0.3350384362442295\n",
            "gradient : -7.252867480088061e-05\n",
            "loss : 0.3344678465783574\n",
            "gradient : -6.801771320982976e-05\n",
            "loss : 0.33391221718331476\n",
            "gradient : -6.357882209025077e-05\n",
            "loss : 0.3333712216605752\n",
            "gradient : -5.9211577991510555e-05\n",
            "loss : 0.33284453774174505\n",
            "gradient : -5.491553683634701e-05\n",
            "loss : 0.33233184727464493\n",
            "gradient : -5.069023567474685e-05\n",
            "loss : 0.33183283621543563\n",
            "gradient : -4.653519429806913e-05\n",
            "loss : 0.3313471946261167\n",
            "gradient : -4.244991672178395e-05\n",
            "loss : 0.33087461667675233\n",
            "gradient : -3.843389254499643e-05\n",
            "loss : 0.33041480065177525\n",
            "gradient : -3.448659819485022e-05\n",
            "loss : 0.3299674489597478\n",
            "gradient : -3.060749806371765e-05\n",
            "loss : 0.32953226814597497\n",
            "gradient : -2.6796045546825357e-05\n",
            "loss : 0.3291089689073942\n",
            "gradient : -2.305168398778264e-05\n",
            "loss : 0.3286972661091829\n",
            "gradient : -1.9373847539097934e-05\n",
            "loss : 0.328296878802559\n",
            "gradient : -1.5761961944658772e-05\n",
            "loss : 0.327907530243284\n",
            "gradient : -1.2215445250685718e-05\n",
            "loss : 0.3275289479103953\n",
            "gradient : -8.733708451426242e-06\n"
          ]
        }
      ]
    }
  ]
}